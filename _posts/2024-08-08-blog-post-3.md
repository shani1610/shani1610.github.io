---
title: 'Diffusion Models Extentions'
date: 2024-08-08
permalink: /posts/2024/08/blog-post-3/
tags:
  - generativeai
---


# Draft Mode

It is good to think about what kind of problem we want to solve. If it involves image-to-image (img2img) tasks, we can take inspiration from super-resolution, colorization, and inpainting, as well as pixel-level image understanding tasks. Then, we should consider task-specific architecture customizations, changes to hyperparameters, or modifications to the loss function.

Here I will explore several popular Diffusion Model (DM) Extensions:  
**ControlNet, Blended Latents, LoRA, Make-A-Scene, and Palette.**

---

## **1. ControlNet**
**Paper:** Adding Conditional Control to Text-to-Image Diffusion Models  
**ArXiv Link:** [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)

by the paper Adding Conditional Control to Text-to-Image Diffusion Models

text-to-image models are limited in the control they
provide over the spatial composition of the image; precisely
expressing complex layouts, poses, shapes and forms can be
difficult via text prompts alone.

add spatial conditioning controls to large, pretrained textto-image diffusion models.

ControlNet locks the productionready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a
strong backbone to learn a diverse set of conditional controls.
locking its parameters,
and also making a trainable copy of its encoding layers.

The
trainable copy and the original, locked model are connected
with zero convolution layers, with weights initialized to zeros
so that they progressively grow during the training.
The neural architecture is connected with “zero convolutions”
(zero-initialized convolution layers) that progressively grow
the parameters from zero and ensure that no harmful noise
could affect the finetuning.

This
architecture ensures that harmful noise is not added to the
deep features of the large diffusion model at the beginning
of training, and protects the large-scale pretrained backbone
in the trainable copy from being damaged by such noise.

The direct finetuning or continued training of a large pretrained model with
limited data may cause overfitting and catastrophic forgetting [31, 75].Researchers have shown that such forgetting
can be alleviated by restricting the number or rank of trainable parameters (the next paper we explore is lora).
designing
deeper or more customized neural architectures might be
necessary for handling in-the-wild conditioning images with
complex shapes and diverse high-level semantics.
One way to finetune a neural network is to directly continue
training it with the additional training data. But this approach
can lead to overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on developing finetuning strategies that avoid such issues.
(HyperNetwork, Adapter, Additive Learning, Low-Rank Adaptation (LoRA) ,Zero-Initialized Layers)

Zero-Initialized Layers are used by ControlNet for connecting network blocks. Research on neural networks has
extensively discussed the initialization and manipulation of
network weights

<img src="/images/dmextensions/controlnet_block.png" alt="drawing" width="400"/>





The way we connect the ControlNet is computationally
efficient — since the locked copy parameters are frozen, no
gradient computation is required in the originally locked
encoder for the finetuning. This approach speeds up training and saves GPU memory.

<img src="/images/dmextensions/controlnet_archi.png" alt="drawing" width="400"/>

To add ControlNet to
Stable Diffusion, we first convert each input conditioning
image (e.g., edge, pose, depth, etc.) from an input size of
512 × 512 into a 64 × 64 feature space vector that matches
the size of Stable Diffusion latent image shape.

Suppose F(·; Θ) is such a trained
neural block, with parameters Θ, that transforms an input
feature map x, into another feature map y as y = F(x; Θ). x and y are usually 2D feature maps
To add a ControlNet to such a pre-trained neural block,
we lock (freeze) the parameters Θ of the original block and
simultaneously clone the block to a trainable copy with
parameters Θc. The trainable copy takes an
external conditioning vector c as input.
The trainable copy is connected to the locked model with
zero convolution layers, denoted Z.
is a 1 × 1 convolution layer with both weight and bias initialized to zeros. To build up a ControlNet, we use two
instances of zero convolutions with parameters Θz1 and Θz2
respectively. The complete ControlNet then computes
yc = F(x; Θ) + Z(F(x + Z(c; Θz1); Θc); Θz2),
where yc is the output of the ControlNet block.
Zero convolutions protect this backbone by eliminating random noise as gradients in the initial
training steps. We detail the gradient calculation for zero
convolutions in supplementary materials.


g. Stable Diffusion depends on a technique called Classifier-Free Guidance (CFG) [29] to generate high-quality images.

from medium:
By starting with zero-initialized weights, the zero convolution layer ensures that the initial outputs are identical to those of the pre-trained model. 
 The use of zero convolution allows for efficient training by not introducing additional noise to the deep features.
only the incremental changes (residuals) are learned.
Q: i still dont understand if the trainable result are being added to the regular results? concanated? how they are combined? 

## **2. Blended Latent Diffusion**
**Paper:** 
**ArXiv Link:** []()

The local editing scenario, where the artist is only
interested in modifying a part of a generic image, while preserving
the remaining parts, has not received nearly as much attention,
we harness the merits of LDM to the task of local
text-guided natural image editing, where the user provides the image
to be edited, a natural language text prompt, and a mask indicating
an area to which the edit should be confined.

Given an image 𝑥, a guiding text prompt 𝑑 and a binary mask𝑚 that
marks the region of interest in the image, our goal is to produce a
modified image 𝑥ˆ, s.t. the content 𝑥ˆ ⊙ 𝑚 is consistent with the text
description 𝑑, while the complementary area remains close to the
source image, i.e., 𝑥 ⊙ (1−𝑚) ≈ 𝑥ˆ ⊙ (1−𝑚), where ⊙ is element-wise
multiplication. Furthermore, the transition between the two areas
of 𝑥ˆ should ideally appear seamless.
we wish to
modify as foreground (fg) and to the remaining part as background
(bg), we follow the idea of Blended Diffusion and repeatedly blend
the two parts in this latent space, as the diffusion progresses. 
We therefore downsample the input mask 𝑚 to these spatial
dimensions to obtain the latent space binary mask 𝑚latent

input image 𝑥 is encoded into the latent space 


we noise the initial latent 𝑧init to the desired noise level
(in a single step) and manipulate the denoising diffusion process in
the following way: at each step, we first perform a latent denoising
step, conditioned directly on the guiding text prompt 𝑑, to obtain
a less noisy foreground latent denoted as 𝑧fg, while also noising
the original latent 𝑧init to the current noise level to obtain a noisy
background latent 𝑧bg. The two latents are then blended using the
resized mask, i.e. 𝑧fg ⊙ 𝑚latent + 𝑧bg ⊙ (1 − 𝑚latent), to yield the
latent for the next latent diffusion step. 
 at each denoising step the entire latent is modified, but the
subsequent blending enforces the parts outside 𝑚latent to remain
the same.

note: this is what i should have done for my video.

<img src="/images/dmextensions/blended_latent_diffusion.png" alt="drawing" width="400"/>

how to keep the unchanged areas good? background preservation

1) A naïve way to deal with this problem is to stitch the original
image and the edited result 𝑥ˆ at the pixel level, using the input mask
𝑚. However, because the unmasked areas were not generated by
the decoder, there is no guarantee that the generated part will blend
seamlessly with the surrounding background. Indeed, this naïve
stitching produces visible seams, as demonstrated in Figure 4(c).

2) Alternatively, one could perform seamless cloning between the
edited region and the original, e.g., utilizing Poisson Image Editing
, which uses gradient-domain reconstruction in
pixel space. However, this often results in a noticeable color shift of
the edited area, as demonstrated in Figure 4(d).
For Poisson image blending [Pérez et al. 2003] we used the
OpenCV [Bradski and Kaehler 2000] implementation
3) In the GAN inversion literature
it is standard practice to achieve image reconstruction via latent-space optimization.
In theory, latent
optimization can also be used to perform seamless cloning, as a
post-process step: given the input image 𝑥, the mask 𝑚, and the
edited image 𝑥ˆ, along with its corresponding latent vector 𝑧0, one
could use latent optimization to search for a better vector 𝑧
∗
, s.t. the
masked area will be similar to the edited image 𝑥ˆ and the unmasked
area will be similar to the input image 𝑥 . using a standard distance metric, such as MSE. 

4) optimizing the decoder’s weights 𝜃 on a per-image basis:: The inability of latent space optimization to capture the highfrequency details suggests that the expressivity of the decoder 𝐷(𝑧)
is limited. This leads us again to draw inspiration from GAN inversion literature — it was shown that fine-tuning the GAN generator
weights per image results in a better reconstruction. Inspired by
this approach, we can achieve seamless cloning by fine-tuning the
decoder’s weights 𝜃 on a per-image basis and use these weights to infer the result 𝑥
∗ = 𝐷𝜃
∗ (𝑧0). As we can
see in Figure 4(f), this method yields the best result: the foreground
region follows 𝑥ˆ, while the background preserves the fine details
from the input image 𝑥, and the blending appears seamless
• For latent optimization and weights optimization we used
Adam optimizer [Kingma and Ba 2014] with a learning rate
of 0.0001 for 75 optimization steps per image.
<img src="/images/dmextensions/background_reconstruction.png" alt="drawing" width="400"/>

## **3. Make-A-Scene**
**Paper:** 
**ArXiv Link:** []()
Our model generates an image given a text input and an optional scene layout
(segmentation map). As demonstrated in our experiments, by conditioning over
the scene layout, our method provides a new form of implicit controllability
To refrain from post-generation filtering and further improve the generation we employ classifier-free guidance.
Scene representation and tokenization
The scene is composed of a union of three complementary semantic segmentation
groups - panoptic, human, and face. By combining the three extracted semantic
segmentation groups, the network learns to both generate the semantic layout
and condition on it while generating the final image. The semantic layout provides additional global context in an implicit form.

## **4. Palette**
**Paper:*Palette: Image-to-Image Diffusion Models* 
**ArXiv Link:** []()

Palette: Image-to-Image Diffusion Models
g image-to-image translation tasks,
namely colorization, inpainting, uncropping, and JPEG restoration.
A natural approach to image-to-image translation
is to learn the conditional distribution of output images given the
input, using deep generative models that can capture multi-modal
distributions in the high-dimensional space of images.
Diffusion models convert
samples from a standard Gaussian distribution into samples from
an empirical data distribution through an iterative denoising process. 
Image-to-image diffusion models are conditional diffusion models
of the form 𝑝(𝒚 | 𝒙), where both 𝒙 and 𝒚 are images, e.g., 𝒙 is a
grayscale image and 𝒚 is a color image.
 Given a
training output image 𝒚, we generate a noisy version 𝒚e, and train a neural network 𝑓𝜃
to denoise 𝒚egiven 𝒙 and a noise level indicator
𝛾, for which the loss is

<img src="/images/dmextensions/palette_loss.png" alt="drawing" width="400"/>

p is 1 or 2, depends if we want L1 or L2 norm. 
We find that 𝐿1 yields
significantly lower sample diversity compared to 𝐿2. While 𝐿1 may
be useful, to reduce potential hallucinations in some applications,
here we adopt 𝐿2 to capture the output distribution more faithfully.
The network
architecture is based on the 256×256 class-conditional U-Net model
of [Dhariwal and Nichol 2021]. The two main differences between
our architecture and theirs are (i) absence of class-conditioning, and
(ii) additional conditioning of the source image via concatenation,
following [Saharia et al. 2021].Image Super-Resolution via Iterative Refinement 
from saharia that is a work on super resolution: . To condition the model on the input x, we up-sample the low-resolution image to the target
resolution using bicubic interpolation. The result is concatenated with yt along the channel dimension. We experimented with more sophisticated methods of conditioning,
such as using FiLM [34], but we found that the simple concatenation yielded similar generation quality
FiLM: Visual Reasoning with a General Conditioning Layer

<img src="/images/dmextensions/palette_algo.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/palette_example.png" alt="drawing" width="400"/>

## **5. SpaText**
**Paper:*## **SpaText: Spatio-Textual Representation for Controllable Image Generation*** 
**ArXiv Link:** []()


<img src="/images/dmextensions/st_train.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/st_inference.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/st_sample.png" alt="drawing" width="400"/>

## **6. diffEdit**
**Paper:** 
**ArXiv Link:** []()

<img src="/images/dmextensions/diffEdit.png" alt="drawing" width="400"/>

# Fine-Tuning\Optimization methods
HyperNetwork, Adapter, Additive Learning, Low-Rank Adaptation (LoRA), Zero-Initialized Layers (like in controlnet)

## **1. LoRa**
**Paper:** 
**ArXiv Link:** []()

These methods use the idea of reparametrizing the weights of the network using a lowrank transformation. This decreases the trainable parameter count while still allowing the
method to work with high-dimensional matrices, such as the pre-trained parameters of the
networks
Parameter update for a weight matrix in LoRA is
decomposed into a product of two low-rank matrices:
δW = WAWB, WA ∈ Rin×r
, WB ∈ Rr×out
.All pre-trained model parameters are kept frozen, and only WA and WB matrices are
trainable. The scaling factor is constant and typically equals 1
r
. After training, they can be
integrated into the original W by just adding the matrix WAWB to the original matrix W.
Pseudocode is very simple:
```
def lora_linear ( x ) :
h = x @ W # regular linear
dh = x @ W_A @ W_B # low - rank update
h += scale * dh # scaling
return h
```
In Transformers, LoRA is typically used for WK and WV projection matrices in multihead attention modules. However, to achieve the best possible performance, it is best to
apply LoRA to all weight matrices in the model

Even though LoRA was initially proposed for large-language models and demonstrated on transformer blocks, the technique can also be applied elsewhere. In the case of Stable Diffusion fine-tuning, LoRA can be applied to the cross-attention layers that relate the image representations with the prompts that describe them.

<img src="/images/dmextensions/transformer.png" alt="drawing" width="400"/>

from: Scaling Down to Scale Up:
A Guide to Parameter-Efficient Fine-Tuning 

<img src="/images/dmextensions/lora.png" alt="drawing" width="400"/>


in code:

from: https://github.com/taldatech/ee046211-deep-learning tutorial 09
normal finetuning:
```
 model_ft = models.vgg16(weights=weights)
 set_parameter_requires_grad(model_ft, feature_extract)
 num_ftrs = model_ft.classifier[6].in_features
 model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)
 input_size = 224
```
finetuning with LoRA:
```
model_ft = models.vgg16(weights=weigths)
# turn off gradients
model_ft.requires_grad_(False)
# replace the penultimate fc layer (a 4096 x 4096 weight matrix) with low-rank adaptation
rank = 4
alpha = 8
model_ft.classifier[3] = LowRankLayer(model_ft.classifier[3], rank, alpha, use_dora=True)
# change the last linear layer to the correct number of classes
num_classes = 2
num_ftrs = model_ft.classifier[6].in_features
model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)
input_size = 224
```

# Add later:

catastrophic forgetting https://towardsdatascience.com/forgetting-in-deep-learning-4672e8843a7f

Cascaded Diffusion Models for High Fidelity Image Generation (Ho et al., 2021): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis

Classifier-Free Diffusion Guidance (Ho et al., 2021): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network

Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) (Ramesh et al., 2022): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image

dreambooth
