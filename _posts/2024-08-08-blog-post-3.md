---
title: 'Diffusion Models Extentions'
date: 2024-08-08
permalink: /posts/2024/08/blog-post-3/
tags:
  - generativeai
---

draft mode

Here I will explore 3 popular DM Extension, 
ControlNet
Blended latents
LoRA

ControlNet
by the paper Adding Conditional Control to Text-to-Image Diffusion Models

text-to-image models are limited in the control they
provide over the spatial composition of the image; precisely
expressing complex layouts, poses, shapes and forms can be
difficult via text prompts alone.

add spatial conditioning controls to large, pretrained textto-image diffusion models.

ControlNet locks the productionready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a
strong backbone to learn a diverse set of conditional controls.
locking its parameters,
and also making a trainable copy of its encoding layers.

The
trainable copy and the original, locked model are connected
with zero convolution layers, with weights initialized to zeros
so that they progressively grow during the training.
The neural architecture is connected with â€œzero convolutionsâ€
(zero-initialized convolution layers) that progressively grow
the parameters from zero and ensure that no harmful noise
could affect the finetuning.

This
architecture ensures that harmful noise is not added to the
deep features of the large diffusion model at the beginning
of training, and protects the large-scale pretrained backbone
in the trainable copy from being damaged by such noise.

The direct finetuning or continued training of a large pretrained model with
limited data may cause overfitting and catastrophic forgetting [31, 75].Researchers have shown that such forgetting
can be alleviated by restricting the number or rank of trainable parameters (the next paper we explore is lora).
designing
deeper or more customized neural architectures might be
necessary for handling in-the-wild conditioning images with
complex shapes and diverse high-level semantics.
One way to finetune a neural network is to directly continue
training it with the additional training data. But this approach
can lead to overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on developing finetuning strategies that avoid such issues.
(HyperNetwork, Adapter, Additive Learning, Low-Rank Adaptation (LoRA) ,Zero-Initialized Layers)

Zero-Initialized Layers are used by ControlNet for connecting network blocks. Research on neural networks has
extensively discussed the initialization and manipulation of
network weights





The way we connect the ControlNet is computationally
efficient â€” since the locked copy parameters are frozen, no
gradient computation is required in the originally locked
encoder for the finetuning. This approach speeds up training and saves GPU memory.

To add ControlNet to
Stable Diffusion, we first convert each input conditioning
image (e.g., edge, pose, depth, etc.) from an input size of
512 Ã— 512 into a 64 Ã— 64 feature space vector that matches
the size of Stable Diffusion latent image shape.

Suppose F(Â·; Î˜) is such a trained
neural block, with parameters Î˜, that transforms an input
feature map x, into another feature map y as y = F(x; Î˜). x and y are usually 2D feature maps
To add a ControlNet to such a pre-trained neural block,
we lock (freeze) the parameters Î˜ of the original block and
simultaneously clone the block to a trainable copy with
parameters Î˜c. The trainable copy takes an
external conditioning vector c as input.
The trainable copy is connected to the locked model with
zero convolution layers, denoted Z.
is a 1 Ã— 1 convolution layer with both weight and bias initialized to zeros. To build up a ControlNet, we use two
instances of zero convolutions with parameters Î˜z1 and Î˜z2
respectively. The complete ControlNet then computes
yc = F(x; Î˜) + Z(F(x + Z(c; Î˜z1); Î˜c); Î˜z2),
where yc is the output of the ControlNet block.
Zero convolutions protect this backbone by eliminating random noise as gradients in the initial
training steps. We detail the gradient calculation for zero
convolutions in supplementary materials.


g. Stable Diffusion depends on a technique called Classifier-Free Guidance (CFG) [29] to generate high-quality images.

from medium:
By starting with zero-initialized weights, the zero convolution layer ensures that the initial outputs are identical to those of the pre-trained model. 
 The use of zero convolution allows for efficient training by not introducing additional noise to the deep features.
only the incremental changes (residuals) are learned.
Q: i still dont understand if the trainable result are being added to the regular results? concanated? how they are combined? 


# Blended Latent Diffusion
The local editing scenario, where the artist is only
interested in modifying a part of a generic image, while preserving
the remaining parts, has not received nearly as much attention,
we harness the merits of LDM to the task of local
text-guided natural image editing, where the user provides the image
to be edited, a natural language text prompt, and a mask indicating
an area to which the edit should be confined.

Given an image ğ‘¥, a guiding text prompt ğ‘‘ and a binary maskğ‘š that
marks the region of interest in the image, our goal is to produce a
modified image ğ‘¥Ë†, s.t. the content ğ‘¥Ë† âŠ™ ğ‘š is consistent with the text
description ğ‘‘, while the complementary area remains close to the
source image, i.e., ğ‘¥ âŠ™ (1âˆ’ğ‘š) â‰ˆ ğ‘¥Ë† âŠ™ (1âˆ’ğ‘š), where âŠ™ is element-wise
multiplication. Furthermore, the transition between the two areas
of ğ‘¥Ë† should ideally appear seamless.
we wish to
modify as foreground (fg) and to the remaining part as background
(bg), we follow the idea of Blended Diffusion and repeatedly blend
the two parts in this latent space, as the diffusion progresses. 
We therefore downsample the input mask ğ‘š to these spatial
dimensions to obtain the latent space binary mask ğ‘šlatent

input image ğ‘¥ is encoded into the latent space 


we noise the initial latent ğ‘§init to the desired noise level
(in a single step) and manipulate the denoising diffusion process in
the following way: at each step, we first perform a latent denoising
step, conditioned directly on the guiding text prompt ğ‘‘, to obtain
a less noisy foreground latent denoted as ğ‘§fg, while also noising
the original latent ğ‘§init to the current noise level to obtain a noisy
background latent ğ‘§bg. The two latents are then blended using the
resized mask, i.e. ğ‘§fg âŠ™ ğ‘šlatent + ğ‘§bg âŠ™ (1 âˆ’ ğ‘šlatent), to yield the
latent for the next latent diffusion step. 
 at each denoising step the entire latent is modified, but the
subsequent blending enforces the parts outside ğ‘šlatent to remain
the same.

note: this is what i should have done for my video.

<img src="/images/dmextensions/blended_latent_diffusion.png" alt="drawing" width="200"/>

how to keep the unchanged areas good? background preservation

1) A naÃ¯ve way to deal with this problem is to stitch the original
image and the edited result ğ‘¥Ë† at the pixel level, using the input mask
ğ‘š. However, because the unmasked areas were not generated by
the decoder, there is no guarantee that the generated part will blend
seamlessly with the surrounding background. Indeed, this naÃ¯ve
stitching produces visible seams, as demonstrated in Figure 4(c).

2) Alternatively, one could perform seamless cloning between the
edited region and the original, e.g., utilizing Poisson Image Editing
, which uses gradient-domain reconstruction in
pixel space. However, this often results in a noticeable color shift of
the edited area, as demonstrated in Figure 4(d).
For Poisson image blending [PÃ©rez et al. 2003] we used the
OpenCV [Bradski and Kaehler 2000] implementation
3) In the GAN inversion literature
it is standard practice to achieve image reconstruction via latent-space optimization.
In theory, latent
optimization can also be used to perform seamless cloning, as a
post-process step: given the input image ğ‘¥, the mask ğ‘š, and the
edited image ğ‘¥Ë†, along with its corresponding latent vector ğ‘§0, one
could use latent optimization to search for a better vector ğ‘§
âˆ—
, s.t. the
masked area will be similar to the edited image ğ‘¥Ë† and the unmasked
area will be similar to the input image ğ‘¥ . using a standard distance metric, such as MSE. 

4) optimizing the decoderâ€™s weights ğœƒ on a per-image basis:: The inability of latent space optimization to capture the highfrequency details suggests that the expressivity of the decoder ğ·(ğ‘§)
is limited. This leads us again to draw inspiration from GAN inversion literature â€” it was shown that fine-tuning the GAN generator
weights per image results in a better reconstruction. Inspired by
this approach, we can achieve seamless cloning by fine-tuning the
decoderâ€™s weights ğœƒ on a per-image basis and use these weights to infer the result ğ‘¥
âˆ— = ğ·ğœƒ
âˆ— (ğ‘§0). As we can
see in Figure 4(f), this method yields the best result: the foreground
region follows ğ‘¥Ë†, while the background preserves the fine details
from the input image ğ‘¥, and the blending appears seamless
â€¢ For latent optimization and weights optimization we used
Adam optimizer [Kingma and Ba 2014] with a learning rate
of 0.0001 for 75 optimization steps per image.
<img src="/images/dmextensions/background_reconstruction.png" alt="drawing" width="200"/>

LoRa
These methods use the idea of reparametrizing the weights of the network using a lowrank transformation. This decreases the trainable parameter count while still allowing the
method to work with high-dimensional matrices, such as the pre-trained parameters of the
networks
Parameter update for a weight matrix in LoRA is
decomposed into a product of two low-rank matrices:
Î´W = WAWB, WA âˆˆ RinÃ—r
, WB âˆˆ RrÃ—out
.All pre-trained model parameters are kept frozen, and only WA and WB matrices are
trainable. The scaling factor is constant and typically equals 1
r
. After training, they can be
integrated into the original W by just adding the matrix WAWB to the original matrix W.
Pseudocode is very simple:
```
def lora_linear ( x ) :
h = x @ W # regular linear
dh = x @ W_A @ W_B # low - rank update
h += scale * dh # scaling
return h
```
In Transformers, LoRA is typically used for WK and WV projection matrices in multihead attention modules. However, to achieve the best possible performance, it is best to
apply LoRA to all weight matrices in the model

<img src="/images/dmextensions/transformer.png" alt="drawing" width="200"/>

from: Scaling Down to Scale Up:
A Guide to Parameter-Efficient Fine-Tuning 



# dreambooth


