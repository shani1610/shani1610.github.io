---
title: 'Diffusion Models Extentions'
date: 2024-08-08
permalink: /posts/2024/08/blog-post-3/
tags:
  - generativeai
---


# Draft Mode

It is good to think about what kind of problem we want to solve. If it involves image-to-image (img2img) tasks, we can take inspiration from super-resolution, colorization, and inpainting, as well as pixel-level image understanding tasks. Then, we should consider task-specific architecture customizations, changes to hyperparameters, or modifications to the loss function.

Here I will explore several popular Diffusion Model (DM) Extensions:  
**ControlNet, Blended Latents, LoRA, Make-A-Scene, and Palette.** update this and make as table of content

---

## **1. ControlNet**
**Paper:** Adding Conditional Control to Text-to-Image Diffusion Models  
**ArXiv Link:** [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)

by the paper Adding Conditional Control to Text-to-Image Diffusion Models

text-to-image models are limited in the control they
provide over the spatial composition of the image; precisely
expressing complex layouts, poses, shapes and forms can be
difficult via text prompts alone.

add spatial conditioning controls to large, pretrained textto-image diffusion models.

ControlNet locks the productionready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a
strong backbone to learn a diverse set of conditional controls.
locking its parameters,
and also making a trainable copy of its encoding layers.

The
trainable copy and the original, locked model are connected
with zero convolution layers, with weights initialized to zeros
so that they progressively grow during the training.
The neural architecture is connected with â€œzero convolutionsâ€
(zero-initialized convolution layers) that progressively grow
the parameters from zero and ensure that no harmful noise
could affect the finetuning.

This
architecture ensures that harmful noise is not added to the
deep features of the large diffusion model at the beginning
of training, and protects the large-scale pretrained backbone
in the trainable copy from being damaged by such noise.

The direct finetuning or continued training of a large pretrained model with
limited data may cause overfitting and catastrophic forgetting [31, 75].Researchers have shown that such forgetting
can be alleviated by restricting the number or rank of trainable parameters (the next paper we explore is lora).
designing
deeper or more customized neural architectures might be
necessary for handling in-the-wild conditioning images with
complex shapes and diverse high-level semantics.
One way to finetune a neural network is to directly continue
training it with the additional training data. But this approach
can lead to overfitting, mode collapse, and catastrophic forgetting. Extensive research has focused on developing finetuning strategies that avoid such issues.
(HyperNetwork, Adapter, Additive Learning, Low-Rank Adaptation (LoRA) ,Zero-Initialized Layers)

Zero-Initialized Layers are used by ControlNet for connecting network blocks. Research on neural networks has
extensively discussed the initialization and manipulation of
network weights

<img src="/images/dmextensions/controlnet_block.png" alt="drawing" width="400"/>

The way we connect the ControlNet is computationally
efficient â€” since the locked copy parameters are frozen, no
gradient computation is required in the originally locked
encoder for the finetuning. This approach speeds up training and saves GPU memory.

<img src="/images/dmextensions/controlnet_archi.png" alt="drawing" width="400"/>

To add ControlNet to
Stable Diffusion, we first convert each input conditioning
image (e.g., edge, pose, depth, etc.) from an input size of
512 Ã— 512 into a 64 Ã— 64 feature space vector that matches
the size of Stable Diffusion latent image shape.

Suppose F(Â·; Î˜) is such a trained
neural block, with parameters Î˜, that transforms an input
feature map x, into another feature map y as y = F(x; Î˜). x and y are usually 2D feature maps
To add a ControlNet to such a pre-trained neural block,
we lock (freeze) the parameters Î˜ of the original block and
simultaneously clone the block to a trainable copy with
parameters Î˜c. The trainable copy takes an
external conditioning vector c as input.
The trainable copy is connected to the locked model with
zero convolution layers, denoted Z.
is a 1 Ã— 1 convolution layer with both weight and bias initialized to zeros. To build up a ControlNet, we use two
instances of zero convolutions with parameters Î˜z1 and Î˜z2
respectively. The complete ControlNet then computes
yc = F(x; Î˜) + Z(F(x + Z(c; Î˜z1); Î˜c); Î˜z2),
where yc is the output of the ControlNet block.
Zero convolutions protect this backbone by eliminating random noise as gradients in the initial
training steps. We detail the gradient calculation for zero
convolutions in supplementary materials.


g. Stable Diffusion depends on a technique called Classifier-Free Guidance (CFG) [29] to generate high-quality images.

from medium:
By starting with zero-initialized weights, the zero convolution layer ensures that the initial outputs are identical to those of the pre-trained model. 
 The use of zero convolution allows for efficient training by not introducing additional noise to the deep features.
only the incremental changes (residuals) are learned.
Q: i still dont understand if the trainable result are being added to the regular results? concanated? how they are combined? 

## **2. Blended Latent Diffusion**
**Paper:** 
**ArXiv Link:** []()

The local editing scenario, where the artist is only
interested in modifying a part of a generic image, while preserving
the remaining parts, has not received nearly as much attention,
we harness the merits of LDM to the task of local
text-guided natural image editing, where the user provides the image
to be edited, a natural language text prompt, and a mask indicating
an area to which the edit should be confined.

Given an image ğ‘¥, a guiding text prompt ğ‘‘ and a binary maskğ‘š that
marks the region of interest in the image, our goal is to produce a
modified image ğ‘¥Ë†, s.t. the content ğ‘¥Ë† âŠ™ ğ‘š is consistent with the text
description ğ‘‘, while the complementary area remains close to the
source image, i.e., ğ‘¥ âŠ™ (1âˆ’ğ‘š) â‰ˆ ğ‘¥Ë† âŠ™ (1âˆ’ğ‘š), where âŠ™ is element-wise
multiplication. Furthermore, the transition between the two areas
of ğ‘¥Ë† should ideally appear seamless.
we wish to
modify as foreground (fg) and to the remaining part as background
(bg), we follow the idea of Blended Diffusion and repeatedly blend
the two parts in this latent space, as the diffusion progresses. 
We therefore downsample the input mask ğ‘š to these spatial
dimensions to obtain the latent space binary mask ğ‘šlatent

input image ğ‘¥ is encoded into the latent space 


we noise the initial latent ğ‘§init to the desired noise level
(in a single step) and manipulate the denoising diffusion process in
the following way: at each step, we first perform a latent denoising
step, conditioned directly on the guiding text prompt ğ‘‘, to obtain
a less noisy foreground latent denoted as ğ‘§fg, while also noising
the original latent ğ‘§init to the current noise level to obtain a noisy
background latent ğ‘§bg. The two latents are then blended using the
resized mask, i.e. ğ‘§fg âŠ™ ğ‘šlatent + ğ‘§bg âŠ™ (1 âˆ’ ğ‘šlatent), to yield the
latent for the next latent diffusion step. 
 at each denoising step the entire latent is modified, but the
subsequent blending enforces the parts outside ğ‘šlatent to remain
the same.

note: this is what i should have done for my video.

<img src="/images/dmextensions/blended_latent_diffusion.png" alt="drawing" width="400"/>

how to keep the unchanged areas good? background preservation

1) A naÃ¯ve way to deal with this problem is to stitch the original
image and the edited result ğ‘¥Ë† at the pixel level, using the input mask
ğ‘š. However, because the unmasked areas were not generated by
the decoder, there is no guarantee that the generated part will blend
seamlessly with the surrounding background. Indeed, this naÃ¯ve
stitching produces visible seams, as demonstrated in Figure 4(c).

2) Alternatively, one could perform seamless cloning between the
edited region and the original, e.g., utilizing Poisson Image Editing
, which uses gradient-domain reconstruction in
pixel space. However, this often results in a noticeable color shift of
the edited area, as demonstrated in Figure 4(d).
For Poisson image blending [PÃ©rez et al. 2003] we used the
OpenCV [Bradski and Kaehler 2000] implementation
3) In the GAN inversion literature
it is standard practice to achieve image reconstruction via latent-space optimization.
In theory, latent
optimization can also be used to perform seamless cloning, as a
post-process step: given the input image ğ‘¥, the mask ğ‘š, and the
edited image ğ‘¥Ë†, along with its corresponding latent vector ğ‘§0, one
could use latent optimization to search for a better vector ğ‘§
âˆ—
, s.t. the
masked area will be similar to the edited image ğ‘¥Ë† and the unmasked
area will be similar to the input image ğ‘¥ . using a standard distance metric, such as MSE. 

4) optimizing the decoderâ€™s weights ğœƒ on a per-image basis:: The inability of latent space optimization to capture the highfrequency details suggests that the expressivity of the decoder ğ·(ğ‘§)
is limited. This leads us again to draw inspiration from GAN inversion literature â€” it was shown that fine-tuning the GAN generator
weights per image results in a better reconstruction. Inspired by
this approach, we can achieve seamless cloning by fine-tuning the
decoderâ€™s weights ğœƒ on a per-image basis and use these weights to infer the result ğ‘¥
âˆ— = ğ·ğœƒ
âˆ— (ğ‘§0). As we can
see in Figure 4(f), this method yields the best result: the foreground
region follows ğ‘¥Ë†, while the background preserves the fine details
from the input image ğ‘¥, and the blending appears seamless
â€¢ For latent optimization and weights optimization we used
Adam optimizer [Kingma and Ba 2014] with a learning rate
of 0.0001 for 75 optimization steps per image.
<img src="/images/dmextensions/background_reconstruction.png" alt="drawing" width="400"/>

## **3. Make-A-Scene**
**Paper:** 
**ArXiv Link:** []()
Our model generates an image given a text input and an optional scene layout
(segmentation map). As demonstrated in our experiments, by conditioning over
the scene layout, our method provides a new form of implicit controllability
To refrain from post-generation filtering and further improve the generation we employ classifier-free guidance.
Scene representation and tokenization
The scene is composed of a union of three complementary semantic segmentation
groups - panoptic, human, and face. By combining the three extracted semantic
segmentation groups, the network learns to both generate the semantic layout
and condition on it while generating the final image. The semantic layout provides additional global context in an implicit form.

## **4. Palette**
**Paper:*Palette: Image-to-Image Diffusion Models* 
**ArXiv Link:** []()

this work for image-2-image tasks as colorization, inpainting, uncropping and cetra, 
is insipired by [Saharia et al. 2021]:Image Super-Resolution via Iterative Refinement.
what they did is adding in concatentation the image x to the noisy image ye. 
so to my understanding if the noise is 3 channnels and the grayscale is 1 channel, so the architecture is being adjusted to input of 4 channels. 
formally, Image-to-image diffusion models are conditional diffusion models
of the form ğ‘(ğ’š | ğ’™), where both ğ’™ and ğ’š are images, e.g. for colorization task, ğ’™ is a
grayscale image and ğ’š is a color image.
 Given a
training output image ğ’š, we generate a noisy version ğ’še, and train a neural network ğ‘“ğœƒ
to denoise ğ’še given ğ’™ and a noise level indicator
ğ›¾, for which the loss is

<img src="/images/dmextensions/palette_loss.png" alt="drawing" width="400"/>

p is 1 or 2, depends if we want L1 or L2 norm.  ğ¿1 yields lower diversity but reduce potential hallucination,
ğ¿2 to capture the output distribution more faithfully.

<img src="/images/dmextensions/palette_algo.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/palette_example.png" alt="drawing" width="400"/>

## **5. SpaText**
**Paper:*## **SpaText: Spatio-Textual Representation for Controllable Image Generation*** 

**ArXiv Link:** []()

<img src="/images/dmextensions/st_train.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/st_inference.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/st_sample.png" alt="drawing" width="400"/>

## **6. diffEdit**
**Paper:*DIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE* 
**ArXiv Link:** [DIFFEDIT](https://arxiv.org/pdf/2210.11427)

diffEdit want to edit an image by text only, without asking the used to provide a mask like other methods. 
but since they want to preserve the background they want to find automically the mask of the desired edited region in the image. 
so in first step, they add 50% gaussian noise to the image, and they denoise it with the original prompt, and with the target prompt, 
they take the noise estimates of these two denoising results and calculate their differance and binarize it to get a mask. 
the idea is that the noise estimate of "horse" will be spaitial similiar to "zebra" which will give us the animal body as the mask.
then they use DDIM Inversion technique to find the noise xr that will yield the original image xt, 
then they take the noisy encoded image and denoise it, while injecting the mask to control the denoising, 
by  Ëœyt = Myt + (1âˆ’M)xt, xt is the inferred latent from xr, M is the mask, yt is the denoised image, so Myt is the edited region, and yt~ is the edited image. 

<img src="/images/dmextensions/diffEdit.png" alt="drawing" width="400"/>

## **6. PROMPT-TO-PROMPT**
**Paper:*PROMPT-TO-PROMPT IMAGE EDITING
WITH CROSS-ATTENTION CONTROL* 
**ArXiv Link:** []()

prompt-to-ptomt is an image editing framework that uses only the target text (doesnt provide any mask) and the idea is to preserve the areas where the isnt a change between the source prompt and target promt. 
the key observation is that the connection between the text embedding and the spatial layout is in the cross attention maps. 
so they say that in the denoising process, the text embedding is the K,V matrices and the deep spatial features of the image are the Q matrix, 
so considering that the attention is given by 

M = Softmax 
QKT
âˆš
d

, 

the cross attention output is Ï•b(zt) = MV. we can look at M as the similariy matrix of text and image, and V is the values, so MV express the weighted avrage of the values.
they observed that each token in the prompt get a corresponding cross attention map that show the spatial layout of that token in the image. 
so they say for eaxpmle for swap words, if we have the image and the source prompt, we can find it cross attention maps for the words that are different in the target prompt, 
and then in the denoising process, we can inject the original image cross attention map Mt ,overriding the M*t, to control the denoising process. 

<img src="/images/dmextensions/p2p_cross_att.png" alt="drawing" width="400"/>

<img src="/images/dmextensions/p2p_cross_control.png" alt="drawing" width="400"/>

others for text conditioned image editing: 
Song et al.
(2021) proposed to condition the generation process by copy-pasting pixel values from the reference
image at each denoising step. 
Nichol et al. (2021) use a similar technique by copy-pasting pixels in
the estimated final version of the image. 
Wang et al. (2022b) use DDIM encoding of the input image,
and then decode on edited sketches or semantic segmentation maps. The gradient of a CLIP score
can also be used to match a given text query inside a mask, as in  blended diffusion.
methods that do not require an editing mask.
In DiffusionCLIP (Kim & Ye,
2021), the weights of the diffusion model themselves are updated via gradient descent from a CLIP
loss with a target text.

---------------------------------------------------------------------------------------------

# Fine-Tuning\Optimization methods
HyperNetwork, Adapter, Additive Learning, Low-Rank Adaptation (LoRA), Zero-Initialized Layers (like in controlnet)

## **1. LoRa**
**Paper:** 
**ArXiv Link:** []()

These methods use the idea of reparametrizing the weights of the network using a lowrank transformation. This decreases the trainable parameter count while still allowing the
method to work with high-dimensional matrices, such as the pre-trained parameters of the
networks
Parameter update for a weight matrix in LoRA is
decomposed into a product of two low-rank matrices:
Î´W = WAWB, WA âˆˆ RinÃ—r
, WB âˆˆ RrÃ—out
.All pre-trained model parameters are kept frozen, and only WA and WB matrices are
trainable. The scaling factor is constant and typically equals 1
r
. After training, they can be
integrated into the original W by just adding the matrix WAWB to the original matrix W.
Pseudocode is very simple:
```
def lora_linear ( x ) :
h = x @ W # regular linear
dh = x @ W_A @ W_B # low - rank update
h += scale * dh # scaling
return h
```
In Transformers, LoRA is typically used for WK and WV projection matrices in multihead attention modules. However, to achieve the best possible performance, it is best to
apply LoRA to all weight matrices in the model

Even though LoRA was initially proposed for large-language models and demonstrated on transformer blocks, the technique can also be applied elsewhere. In the case of Stable Diffusion fine-tuning, LoRA can be applied to the cross-attention layers that relate the image representations with the prompts that describe them.

<img src="/images/dmextensions/transformer.png" alt="drawing" width="400"/>

from: Scaling Down to Scale Up:
A Guide to Parameter-Efficient Fine-Tuning 

<img src="/images/dmextensions/lora.png" alt="drawing" width="400"/>


in code:

from: https://github.com/taldatech/ee046211-deep-learning tutorial 09
normal finetuning:
```
 model_ft = models.vgg16(weights=weights)
 set_parameter_requires_grad(model_ft, feature_extract)
 num_ftrs = model_ft.classifier[6].in_features
 model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)
 input_size = 224
```
finetuning with LoRA:
```
model_ft = models.vgg16(weights=weigths)
# turn off gradients
model_ft.requires_grad_(False)
# replace the penultimate fc layer (a 4096 x 4096 weight matrix) with low-rank adaptation
rank = 4
alpha = 8
model_ft.classifier[3] = LowRankLayer(model_ft.classifier[3], rank, alpha, use_dora=True)
# change the last linear layer to the correct number of classes
num_classes = 2
num_ftrs = model_ft.classifier[6].in_features
model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)
input_size = 224
```

# Add later:

catastrophic forgetting https://towardsdatascience.com/forgetting-in-deep-learning-4672e8843a7f

Cascaded Diffusion Models for High Fidelity Image Generation (Ho et al., 2021): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis

Classifier-Free Diffusion Guidance (Ho et al., 2021): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network

Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) (Ramesh et al., 2022): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image

dreambooth
