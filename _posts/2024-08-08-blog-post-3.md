---
title: 'Diffusion Models Extensions'
date: 2024-08-08
permalink: /posts/2024/08/blog-post-3/
tags:
  - generativeai
---

# **Draft Mode**

It is good to think about what kind of problem we want to solve. If it involves image-to-image (img2img) tasks, we can take inspiration from **super-resolution**, **colorization**, and **inpainting**, as well as **pixel-level image understanding tasks**. Then, we should consider task-specific architecture customizations, changes to hyperparameters, or modifications to the loss function.

## **Table of Contents**
1. [ControlNet](#1-controlnet)
2. [Blended Latent Diffusion](#2-blended-latent-diffusion)
3. [Palette](#3-palette)
4. [SpaText](#4-spatext)
5. [DiffEdit](#5-diffedit)
6. [Prompt-to-Prompt](#6-prompt-to-prompt)
7. [Fine-Tuning and Optimization](#fine-tuning-and-optimization-methods)

---

## **1. ControlNet**
**Paper:** Adding Conditional Control to Text-to-Image Diffusion Models  
**ArXiv Link:** [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)

ControlNet provides spatial composition control to text-to-image diffusion models. 

- **Key Concepts:**
  - Uses a large model trained on billions of images, **locks it**, and adds a **trainable copy**.
  - The pre-trained locked model and the trainable copy are connected via **zero convolution layers**:
    - Initialized to zero and grow during training.
    - Ensures the initial outputs are identical to the pre-trained model.
    - Only incremental changes (residuals) are learned, preventing harmful noise during fine-tuning.
  - Freezing the original model reduces memory usage by eliminating gradient storage for the locked parameters.

<img src="/images/dmextensions/controlnet_block.png" alt="drawing" width="400"/>
<img src="/images/dmextensions/controlnet_archi.png" alt="drawing" width="400"/>

- **Mathematical Overview:**
  Suppose \(F(·; Θ)\) is a trained neural block with parameters Θ, transforming an input feature map \(x\) into another feature map \(y\).  
  - The trainable copy takes an external conditioning vector \(c\) as input.
  - The model computes:
    \[
    y_c = F(x; Θ) + Z(F(x + Z(c; Θ_{z1}); Θ_c); Θ_{z2}),
    \]
    where \(y_c\) is the output of the ControlNet block.

Further insights are available in the [ControlNet FAQ](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md).

---

## **2. Blended Latent Diffusion**
**Paper:** Blended Latent Diffusion  
**ArXiv Link:** [https://arxiv.org/abs/2206.02779](https://arxiv.org/abs/2206.02779)

Blended Latent Diffusion allows users to modify specific regions of an image based on a mask and a guiding text prompt.

- **Problem Formulation:**
  Given:
  - An image \(x\),
  - A guiding text prompt \(d\),
  - A binary mask \(m\) indicating the region to edit,
  
  The goal is to produce a modified image \(\hat{x}\) such that:
  - \(\hat{x} \odot m\) aligns with \(d\),
  - \(\hat{x} \odot (1 - m) \approx x \odot (1 - m)\).

<img src="/images/dmextensions/blended_latent_diffusion.png" alt="drawing" width="400"/>

- **Key Steps:**
  - Encode \(x\) into latent space and noise it to the desired level.
  - Denoise:
    - Obtain foreground latent (\(z_{\text{fg}}\)) conditioned on \(d\).
    - Generate noisy background latent (\(z_{\text{bg}}\)).
    - Blend the two latents using the resized mask:
      \[
      z = z_{\text{fg}} \odot m_{\text{latent}} + z_{\text{bg}} \odot (1 - m_{\text{latent}})
      \]

- **Background Preservation Techniques:**
  1. **Naïve Stitching:** Simply stitches the edited and unedited parts; often results in visible seams.
  2. **Poisson Image Blending:** Uses gradient-domain reconstruction; can cause color shifts.
  3. **Latent Optimization:** Searches for a latent vector \(z^*\) that balances foreground and background fidelity.
  4. **Per-Image Decoder Fine-Tuning:** Fine-tunes decoder weights on a per-image basis for seamless integration.

---

## **3. Palette**
**Paper:** Palette: Image-to-Image Diffusion Models  
**ArXiv Link:** [https://arxiv.org/abs/2111.05826](https://arxiv.org/abs/2111.05826)

Palette is designed for tasks like **colorization**, **inpainting**, and **uncropping**.

- **Conditional Formulation:**
  - Models the conditional distribution \(p(y | x)\), where both \(x\) and \(y\) are images (e.g., grayscale and color images).
  - The noisy image \(y_e\) is concatenated with the input \(x\), forming a multi-channel input for the diffusion process.

<img src="/images/dmextensions/palette_loss.png" alt="drawing" width="400"/>
<img src="/images/dmextensions/palette_algo.png" alt="drawing" width="400"/>

---

## **4. SpaText**
**Paper:** SpaText: Spatio-Textual Representation for Controllable Image Generation  
**ArXiv Link:** [https://arxiv.org/abs/2208.11910](https://arxiv.org/abs/2208.11910)

SpaText enables controllable image generation by combining spatial and textual conditions.

<img src="/images/dmextensions/st_train.png" alt="drawing" width="400"/>
<img src="/images/dmextensions/st_inference.png" alt="drawing" width="400"/>
<img src="/images/dmextensions/st_sample.png" alt="drawing" width="400"/>

---

## **5. DiffEdit**
**Paper:** DIFFEDIT: Diffusion-Based Semantic Image Editing with Mask Guidance  
**ArXiv Link:** [https://arxiv.org/abs/2210.11427](https://arxiv.org/abs/2210.11427)

DiffEdit automates mask generation by comparing noise estimates from different text conditionings.

<img src="/images/dmextensions/diffEdit.png" alt="drawing" width="400"/>

---

## **6. Prompt-to-Prompt**
**Paper:** Prompt-to-Prompt Image Editing with Cross-Attention Control  
**ArXiv Link:** [https://arxiv.org/abs/2208.01626](https://arxiv.org/abs/2208.01626)

Prompt-to-Prompt preserves unchanged areas by controlling cross-attention maps during the denoising process.

<img src="/images/dmextensions/p2p_cross_att.png" alt="drawing" width="400"/>
<img src="/images/dmextensions/p2p_cross_control.png" alt="drawing" width="400"/>

---

## **Fine-Tuning and Optimization Methods**

Fine-tuning large generative models is challenging due to risks like **overfitting** and **catastrophic forgetting**.

### **1. LoRA (Low-Rank Adaptation)**
- **Paper:** Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning  
  **ArXiv Link:** [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)

- LoRA reparameterizes weight updates into low-rank matrices:
  \[
  \Delta W = W_A W_B, \quad W_A \in \mathbb{R}^{n \times r}, \quad W_B \in \mathbb{R}^{r \times m}
  \]

- **Applications in Stable Diffusion:** Used for fine-tuning cross-attention layers.

<img src="/images/dmextensions/lora.png" alt="drawing" width="400"/>

---

Additional topics to explore:
- **Catastrophic Forgetting:** [Toward Data Science](https://towardsdatascience.com/forgetting-in-deep-learning-4672e8843a7f)
- **Cascaded Diffusion Models (Ho et al., 2021):** Multi-resolution diffusion for high-fidelity synthesis.
- **Classifier-Free Guidance (Ho et al., 2021):** Joint training of conditional and unconditional diffusion models.
- **DreamBooth:** Fine-tuning for personalized image generation.
